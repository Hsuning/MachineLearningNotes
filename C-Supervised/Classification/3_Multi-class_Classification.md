# Multi-class Classification
#MulticlassClassification
```toc
```
## Definition
- A classification problem: target y can take on more than two possible values

![](Pasted%20image%2020230103120939.png)

## Use Cases
- 10 possible digits
- Any of 3 or 5 possible diseases
- Visual defect inspection of parts manufactured in the factory
- Take in photos and classify subjects in the photos as {dog,cat,horse,other}
- Take in a sentence and classify the 'parts of speech' of its elements: {noun, verb, adjective etc..}

## Softmax Regression

### Algorithm
#SoftmaxRegression #SoftmaxActivationFunction
- a generalization of the #LogisticRegression, N outputs are generated and 1 output is selected as the predicted category
- A vector $\vec{z}$  is generated by a linear function which is applied to a softmax function
$$
z_j = \vec{w}_j \cdot \vec{x} + b_j => j = 1,…,N
$$

- The softmax function converts $\vec{z}$ into a probability distribution
	- $a1 = P(y=1|\vec{x})$ : the average estimate of the chance of y being equal to one, given the input features x.
	- The exponential magnifies small differences in the values
	- the output values sum to one
	- The softmax spans all of the outputs. A change in `z0` for example will change the values of `a0`-`a3`. Compare this to other activations such as ReLU or Sigmoid which have a single input and single output.

$$
a_j = \frac {e^{z_j}} {\sum_{k=1}^{N} e^{z_k}} = P(y-j|\vec{x})
$$
```
def my_softmax(z):
    ez = np.exp(z)              #element-wise exponenial
    sm = ez/np.sum(ez)
    return(sm)
```
- Each output will be between 0 and 1 and the outputs will add to 1
- The output $\mathbf{a}$ is a vector of length N, so for softmax regression. 
$$
\begin{align}
\mathbf{a}(x) =
\begin{bmatrix}
P(y = 1 | \mathbf{x}; \mathbf{w},b) \\
\vdots \\
P(y = N | \mathbf{x}; \mathbf{w},b)
\end{bmatrix}
=
\frac{1}{ \sum_{k=1}^{N}{e^{z_k} }}
\begin{bmatrix}
e^{z_1} \\
\vdots \\
e^{z_{N}} \\
\end{bmatrix} \tag{2}
\end{align}
$$
- The larger inputs will correspond to larger output probabilities.
- when applying sofmax with n = 2, resulting same as logistic regression, the parameters are a bit different but ends up reducing to a logistic regression model

### Cost Function
![](Pasted%20image%2020230103125801.png)
- The loss function:
	- a: output of a softmax function = the probabilities that sum to one
	- y: target category
$$
\begin{equation}
  L(\mathbf{a},y)=\begin{cases}
    -log(a_1), & \text{if $y=1$}.\\
        &\vdots\\
     -log(a_N), & \text{if $y=N$}
  \end{cases} \tag{3}
\end{equation}
$$
- Only the line that corresponds to the target contributes to the loss, other lines are zero
	> y in each training example can take on only 1 value. So we only compute $-log \text{ } a_j$  for the actual value of y = j in that particular training example
$$\mathbf{1}\{y == n\} = =\begin{cases}
    1, & \text{if $y==n$}.\\
    0, & \text{otherwise}.
  \end{cases}$$
- the smaller $a_j$ is, the bigger the loss
- Now the cost is:
	- m: number of examples
	- N: number of outputs
$$
\begin{align}
J(\mathbf{w},b) = -\frac{1}{m} \left[ \sum_{i=1}^{m} \sum_{j=1}^{N}  1\left\{y^{(i)} == j\right\} \log \frac{e^{z^{(i)}_j}}{\sum_{k=1}^N e^{z^{(i)}_k} }\right] \tag{4}
\end{align}
$$

#SparseCategoricalCrossentropy 
- Sparse: Y can only take on one of these 10 values
- Categorical: still classify Y into categories

## Neural Network
- A network with multiple units in its final layer. Each output = a category
- An input --> the output with the highest value is the category predicted
- If the output is applied to a softmax function, the output of the softmax will provide probabilities of the input being in each category

### TF with Softmax Output
- Handwritten digit classification with 10 classes, the output layer will have 10 output units, called softmax output layer
- For other activations functions like sigmoid or linear, a1 was a function of z1. However, for softmax, each activation values depends on all of the values of Z

> Better version later (don't use the version shown below)

**Step 1 - Specify the model**
$f_{\vec{w}, b}(\vec{x})$
```
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense

model = Sequential([
	Dense(units=25, activation='relu'),
	Dense(units=15, activation='relu'),
	Dense(units=10, activation='softmax')
	# Change to linear for more accurately implementation (see below)
])
```
**Step 2 - Specify loss and cost**
$L(f_{\vec{w}, b}(\vec{x}), y)$
```
from tensorflow.keras.losses import SparseCategoricalCrossentropy

model.compile(loss=SparseCategoricalCrossentropy)
# use model.compile(loss=SparseCrossEntropy(from_logits=True)) for more accurately implementation (see below)
```

**Step 3 - Train on data to minimize $J(\vec{w}, b)$**
```
model.fit(X, Y, epochs=100)
```

### Numerical Roundoff Errors
#NumericalRoundoffErrors
```
x1 = 2/10000 
# result: 0.0002
x2 = ( 1 + (1/10000) ) - ( 1 - (1/10000) ) # equals to x1 
# result: 0.0001999
```
- computer has only a finite amount of memory to store each number (floating point number)
- the result can have more or less numerical round off error, depending on how you decide to compute the value $2/10000$

### More numerically accurate implementation of logistic loss
```
# Model
model = Sequential([
	Dense(units=25, activation='relu'),
	Dense(units=15, activation='relu'),
	Dense(units=1, activation='linear')
])

# Loss
model.compile(loss=Binarycrossentropy(from_logits=True))

# Fit
model.fit(X, Y, epochs=100)

# Predict
logits = model(X)
f_x = tf.nn.softmax(logits)

```
![](Pasted%20image%2020230104094159.png)
- more numerically accurate implementation of logistic loss: a different way of formulating it that reduces these numerical round-off errors
- specify the $a$ expression directly as the loss function, give tensorflow more flexibility on how to compute this and whether or not it wants to compute a explicity
- set the output layer to use "linear" activation function
- `from_logits=True` **logit =z **: put both the activation function g(z) and cross-entropy loss into the specification of the loss function

### More numerically accurate implementation of Softmax
- the output layer uses a `linear` rather than a `softmax` activation. 
- While it is possible to include the softmax in the output layer, it is more numerically stable if linear outputs are passed to the loss function during training. 
- If the model is used to predict probabilities, the softmax can be applied at that point.

```
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.losses import SparseCategoricalCrossentropy

# Model
model = Sequential([
	Dense(units=25, activation='relu'),
	Dense(units=15, activation='relu'),
	Dense(units=10, activation='linear')
])

# Loss
model.compile(
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),  #<-- Note
    optimizer=tf.keras.optimizers.Adam(0.001),
)

# Fit
model.fit(X, Y, epochs=100)

# Predict
logits = model(X)
f_x = tf.nn.softmax(logits)

```
- Specify the loss with formula $\frac{e^{z_1}}{e^{z_1}+...+e^{z_n}}$, gives TF the ability to rearrange terms and compute this in a more numerically accurate way
- Ex: when the disease is really small, $e^{-z}$ becomes very small, if one of the disease is very large, then its $e^{-z}$ can become a very large number. Rearranging terms can avoid some of these very small / large numbers
- Setting `from_logits=True` as an argument to the loss function specifies that the output activation was linear rather than a softmax
- The final layer don't output probabilities a1, ..., a10, it is instead outputting z1, ..., z10 that can range from large negative numbers to large positive numbers
- To select the most likely category, the softmax is not required. One can find the index of the largest output using [np.argmax()](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html).
- The output must be sent through a [softmax](https://www.tensorflow.org/api_docs/python/tf/nn/softmax). when performing a prediction that expects a probability

## SparseCategorialCrossentropy or CategoricalCrossEntropy
- SparseCategorialCrossentropy: expects the target to be an integer corresponding to the index. For example, if there are 10 potential target values, y would be between 0 and 9. 
- CategoricalCrossEntropy: Expects the target value of an example to be one-hot encoded where the value at the target index is 1 while the other N-1 entries are zero. An example with 10 potential target values, where the target is 2 would be [0,0,1,0,0,0,0,0,0,0].

### Explanation

**Layer 1 - Function of Units 0 and 1**
![](Pasted%20image%2020230105105818.png)
- Inputs = axis = $x_0,x_1$
- Output of the unit = color of the background, indicated by the color bar on the right. ReLu units so outputs don't necessarily fall into 0-1
- The intensity of the background color indicates the highest values
- Contour lines: the transition point between the output, $a^{[1]}_j$ being zero and non-zero
- Contour lines is the inflection point in the ReLu, if z < 0, then g(z) = 0, else g(z) = z
- Unit 1: split classes (0, 1) from (2, 3) / Unit 2 : split classes (0, 2) from (1, 3)
- This layer has created a new set of features for evaluation by the 2nd layer

**Layer 2 - the output layer**
![](Pasted%20image%2020230105110403.png)
- dots = training examples translated by the first layer
- Inputs = axis = outputs of the previous layer $a^{[1]}_0$ and $a^{[1]}_1$
- classes 0 and 1 (blue and green) have  $a^{[1]}_0 = 0$ while classes 0 and 2 (blue and orange) have $a^{[1]}_1 = 0$
- Unit 0 will produce its maximum value for values near (0,0), where class 0 (blue) has been mapped.  
- Unit 1 produces its highest values in the upper left corner selecting class 1 (green).  
- Unit 2 targets the lower right corner where class 2 (orange) resides.  
- Unit 3 produces its highest values in the upper right selecting our final class (purple).
- The values have been coordinated between the units. The unit must produce a maximum value for the class it is selecting for, that value must also be the highest value of all the units for points in that class. This is done by the implied softmax function that is part of the loss function (`SparseCategoricalCrossEntropy`). Unlike other activation functions, the softmax works across all the outputs.