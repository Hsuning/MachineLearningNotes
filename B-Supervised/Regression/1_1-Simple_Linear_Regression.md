#LinearRegression #LinearRegressionOneVariable #Supervised

# Linear Regression with One Variable
```toc
```

## Concept
```
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚training setâ”‚   features
             â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜   targets
                   â”‚
                   â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚learning algorithmsâ”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â”‚
 size        â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”       estimated price
   x  â”€â”€â”€â”€â–º  â”‚f (function)â”‚ â”€â”€â”€â”€â–º  y-hat
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
feature        model             prediction
                                (estimated y)
```

- Also called univariate (only one target variable) linear regression
- Only take a single feature x, and output the estimated $\hat y$
- $f(x) = wx+b$
	- Linear function maps from x to y
	- $f_{w,b}(x) = wx+b$: we can drop $w, b$ and just write $f(x)$
	- $w,b$: #Parameter, w for weight, b for bias

## Notation
![[supervised-1-3.png]]

```
def compute_model_output(x, w, b):
    """
    Computes the prediction of a linear model
    Args:
      x (ndarray (m,)): Data, m examples 
      w,b (scalar)    : model parameters  
    Returns
      y (ndarray (m,)): target values
    """
    m = x.shape[0]
    f_wb = np.zeros(m)
    for i in range(m):
        f_wb[i] = w * x[i] + b
        
    return f_wb

# Make a prediction
w = 200                         
b = 100    
x_i = 1.2  # house with 1200 sqft, units of x are in 1000's
cost_1200sqft = w * x_i + b    
print(f"${cost_1200sqft:.0f} thousand dollars")
```

[Model_Representation](.PythonCodes/Supervised_1_2/C1_W1_Lab03_Model_Representation_Soln.ipynb)

## Cost Function
#CostFunction #MeanSquaredErrorCostFunction #SquaredErrorCostFunction
>The choice of (ð‘¤,ð‘) that fits your data the best is the one that has the smallest cost ð½(ð‘¤,ð‘)

- Provide a measure of the error between our predictions and the actual data $y^{(i)}$
- Use to compare how one choice of $(w, b)$ is better or worse than another choice
- Minimizing the cost can provide optimal values of $w, b$
- The measure is called the _cost_, $J(w,b)$. In training, we measure the cost over all of our training samples $X, y$
- Goal -> Find $w,b$: $\hat y^{(i)}$ is close to $y^{(i)}$ for all $(x^{(i)}, y^{(i)})$
- $J(w,b) = \frac{1}{2m} \sum_{i=1}^m (\hat y - y)^2$
	- $\hat y - y$: error, difference between the target value and the prediction
	- $m$ : number of training examples
	- $\sum$: sum up to measure the error across the entire training set
	- $\frac{1}{m}$ : compute the average squared error so cost function will not automatically get bigger as the training set size gets larger
	- $\frac{1}{2m}$: make calculation a bit neater
	- $J(w,b)$: cost function
- always end up with a bow shape or hammock shape

```
def compute_cost(x, y, w, b): 
    """
    Computes the cost function for linear regression.
    
    Args:
      x (ndarray (m,)): Data, m examples 
      y (ndarray (m,)): target values
      w,b (scalar)    : model parameters  
    
    Returns
        total_cost (float): The cost of using w,b as the parameters for linear regression
               to fit the data points in x and y
    """
    # number of training examples
    m = x.shape[0] 
    
    cost_sum = 0 
    for i in range(m): 
        f_wb = w * x[i] + b   
        cost = (f_wb - y[i]) ** 2  
        cost_sum = cost_sum + cost  
    total_cost = (1 / (2 * m)) * cost_sum  

    return total_cost
```

[Cost_function_Representation](.PythonCodes/Supervised_1_2/C1_W1_Lab04_Cost_function_Soln.ipynb)

## Goal
> The goal of linear regression is to find the parameters w or (w, b) that results in the smallest possible value J(w)

### General Case
- Model: $f_{w,b}(x) = wx+b$
- Parameters: $w, b$
- Cost function: $J(w,b) = \frac{1}{2m} \sum_{i=1}^m (f_{w,b}(x^{(i)}) - y)^2$ = $J(w,b) = \frac{1}{2m} \sum_{i=1}^m (wx^{(i)}+b - y^{(i)})^2$
- **Goal of linear regression**: $minimize_{w,b}J(w,b)$
- Visualization: 3D plot or contour plot (a graphical technique for representing a 3-dimensional surface by plotting constant z slices, called contours, on a 2-dimensional format)  
![[supervised-1.png]]
- Choose w, b that the cost is close to the center of the small ellipse (minimum)

### Simplified Case --> B = 0
- Model: $f_{w}(x) = wx$
- Parameters: $w$
- Cost function: $J(w) = \frac{1}{2m} \sum_{i=1}^m (f_{w}(x^{(i)}) - y)^2$
- **Goal of linear regression**: $minimize_{w}J(w)$
- Visualization:

```
                J(w)
         â”‚ Function of w
         â”‚ based on fw(x)
       O â”‚               O
       O â”‚               O
      4 Oâ”‚              O
         O             O
      3  â”‚O           O
 J(w)    â”‚ O         O
      2  â”‚  O       O
         â”‚   O     O
      1  â”‚    O   O
         â”‚     O O
      0 â”€â”´â”€â”€â”€â”€â”€â”€*â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ w
         0 0.5 1.0 1.5 2.0 2
```

- Choose w to minimize $J(w)$
	- choose the value of w that causes $J(w)$ to be as small as possible
	- ex: w=1 here (\*)

## Gradient Descent
> Use gradient descent to find the value $(w,b)$ that gets the smallest possible cost $J(w,b)$

### Concept
- #GradientDescent with #SquaredErrorCostFunction  
- Repeated steps to adjust the value of parameter $(w, b)$ to gradually get a smaller cost $J(w,b)$
repeat until convergence {  
$w=w-\alpha*\frac{d}{dw}J(w,b) = w - \alpha\frac{1}{m}\sum_{i=0}^{m-1}(f_{w,b}(x^{(i)})-y^{(i)})x^{(i)}$  
$b = b-\alpha*\frac{d}{db}J(w,b) = b - \alpha\frac{1}{m}\sum_{i=0}^{m-1}(f_{w,b}(x^{(i)})-y^{(i)})$  
}  
- _use i=0 and m-1 in codes_  
- _update w and b simultaneously_  

### Partial Derivative
By the rule of calculus:  
![[supervised-1-2.png]]

### Squared Error Cost Function
- As #SquaredErrorCostFunction ( #CovexFunction with **bowl shape**) is used here.
	- Due to the **bowl shape**, the derivatives will always lead a gradient descent toward the bottom, where the gradient is zero.
	- The cost function does not and will never have multiple #LocalMinima. **It has only a single #GlobalMinima and will always converge to this point**

- Implementing derivative part ($\frac{d*J(w,b)}{dw}$, $\frac{d*J(w,b)}{db}$) above

```
def compute_gradient(x, y, w, b): 
    """
    Computes the gradient for linear regression 
    Args:
      x (ndarray (m,)): Data, m examples 
      y (ndarray (m,)): target values
      w,b (scalar)    : model parameters  
    Returns
      dj_dw (scalar): The gradient of the cost w.r.t. the parameters w
      dj_db (scalar): The gradient of the cost w.r.t. the parameter b     
     """
    
    # Number of training examples
    m = x.shape[0]    
    dj_dw = 0
    dj_db = 0
    
    for i in range(m):  
        f_wb = w * x[i] + b 
        dj_dw_i = (f_wb - y[i]) * x[i] 
        dj_db_i = f_wb - y[i] 
        dj_db += dj_db_i
        dj_dw += dj_dw_i 
    dj_dw = dj_dw / m 
    dj_db = dj_db / m 
        
    return dj_dw, dj_db
```

- Gradient descent, utilizing compute_gradient and compute_cost

```
def gradient_descent(x, y, w_in, b_in, alpha, num_iters, cost_function, gradient_function): 
    """
    Performs gradient descent to fit w,b. Updates w,b by taking 
    num_iters gradient steps with learning rate alpha
    
    Args:
      x (ndarray (m,))  : Data, m examples 
      y (ndarray (m,))  : target values
      w_in,b_in (scalar): initial values of model parameters  
      alpha (float):     Learning rate
      num_iters (int):   number of iterations to run gradient descent
      cost_function:     function to call to produce cost
      gradient_function: function to call to produce gradient
      
    Returns:
      w (scalar): Updated value of parameter after running gradient descent
      b (scalar): Updated value of parameter after running gradient descent
      J_history (List): History of cost values
      p_history (list): History of parameters [w,b] 
      """
    
    w = copy.deepcopy(w_in) # avoid modifying global w_in
    # An array to store cost J and w's at each iteration primarily for graphing later
    J_history = []
    p_history = []
    b = b_in
    w = w_in
    
    for i in range(num_iters):
        # Calculate the gradient and update the parameters using gradient_function
        dj_dw, dj_db = gradient_function(x, y, w, b)     

        # Update Parameters using equation (3) above
        b = b - alpha * dj_db                            
        w = w - alpha * dj_dw                            

        # Save cost J at each iteration
        if i<100000:      # prevent resource exhaustion 
            J_history.append(cost_function(x, y, w , b))
            p_history.append([w,b])
        # Print cost every at intervals 10 times or as many iterations if < 10
        if i% math.ceil(num_iters/10) == 0:
            print(f"Iteration {i:4}: Cost {J_history[-1]:0.2e} ",
                  f"dj_dw: {dj_dw: 0.3e}, dj_db: {dj_db: 0.3e}  ",
                  f"w: {w: 0.3e}, b:{b: 0.5e}")
 
    return w, b, J_history, p_history #return w and J,w history for graphing
```

[Gradien_Descent_Representation](.PythonCodes/Supervised_1_2/C1_W1_Lab05_Gradient_Descent_Soln.ipynb)
